Algorithm | Year | Note | Author(s)
------ | ------ | --- | ------
[[Q-learning]] | 1989 | | Watkins et al.
[[SARSA]] | 1994 | | Rummery et al.
[[REINFORCE]] | 1992 | | Williams et al. 
[[DQN (Deep Q-Network)]] | 2013 | | Mnih et al.
[[DDPG (Deep Deterministic Policy Gradient)]] | 2015 | | Lillicrap et al.
[[Double DQN]] | 2015 | | Hasselt et al.
[[Dueling Network]] | 2016 | | Wang et al.
[[DRQN (Deep Recurrent Q-Network)]] | 2017 | | Hausknecht et al.
PPO |
[[BCQ (Batch-Constrained Q-learning)]] | 2019 | offline | Scott et al.
[[A3C (Asynchronous Advantage Actor-Critic)]] | 2016 | | Mnih et al.
[[Stabalised Experience Replay]] | 2018 | technique, MARL | | Foerster et al.
[[QMIX (Q Mixing)]] | 2018 | MARL | Rashid et al.
[[MAAC (Multi-Actor-Attention-critic)]] | 2019 | MARL | Iqbal et al.
[[MADDPG (Multi-agent DDPG)]] | 2020 | MARL | Lowe et al.
[[MABCQ (Multi-agent BCQ)]] | 2021 | MARL | 
[[QTRAN (Q Transformation)]] | | MARL | 
 
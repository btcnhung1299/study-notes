- Moore-Penson pseudo inverse: $(A^\intercal A)^{-1} A^\intercal$
- Approaches to solve linear equation systems in real world (with many equations):
	- Ricardson method
	- Jacobi method
	- Gaus-Seidel method
	- Other over-relaxation method such as conjugate gradients, generalised minimal residual, biconjugate gradients
- Variational Inference
- Shapley value
- K-dimensional simplex
- Entropy regularation 
- ELBO

Multi-armed bandits problemw

Paper: using federated learning to communicate aming agents in MARL

Boltzmann showed that this definition of entropy was equivalent to the thermodynamic entropy to within a constant factor—known as [Boltzmann's constant](https://en.wikipedia.org/wiki/Boltzmann%27s_constant "Boltzmann's constant")In statistical mechanics, entropy is a measure of the number of ways a system can be arranged, ofte n taken to be a measure of "disorder" (the higher the entropy, the higher the disorder).[[25]](https://en.wikipedia.org/wiki/Entropy#cite_note-McH-25)[[26]](https://en.wikipedia.org/wiki/Entropy#cite_note-Sethna78-26)[[27]](https://en.wikipedia.org/wiki/Entropy#cite_note-27) This definition describes the entropy as being proportional to the natural logarithm of the number of possible microscopic configurations of the individual atoms and molecules of the system ([microstates](https://en.wikipedia.org/wiki/Microstate_(statistical_mechanics) "Microstate (statistical mechanics)")) that could cause the observed macroscopic state ([macrostate](https://en.wikipedia.org/wiki/Macrostate "Macrostate")) of the system. The constant of proportionality is the [Boltzmann constant](https://en.wikipedia.org/wiki/Boltzmann_constant "Boltzmann constant").


This definition assumes that the basis set of states has been picked so that there is no information on their relative phases. In a different basis set, the more general expression is

![{\displaystyle S=-k_{\mathrm {B} }\operatorname {Tr} ({\widehat {\rho }}\log({\widehat {\rho }})),}](https://wikimedia.org/api/rest_v1/media/math/render/svg/b4ad656e2c86dbcafa84d41c229bcf1eb4002fde)

where ![{\widehat {\rho }}](https://wikimedia.org/api/rest_v1/media/math/render/svg/976d174c69efc9eaedae9a14add48754666b479e) is the [density matrix](https://en.wikipedia.org/wiki/Density_matrix "Density matrix"), ![\operatorname {Tr}](https://wikimedia.org/api/rest_v1/media/math/render/svg/37afaa40330d0e6a3eff1767cf6bad007f56412e) is [trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra) "Trace (linear algebra)") and ![\log ](https://wikimedia.org/api/rest_v1/media/math/render/svg/79e4debd0ab1c6ce342d0172a7643733305c37bc) is the [matrix logarithm](https://en.wikipedia.org/wiki/Matrix_logarithm "Matrix logarithm"). This density matrix formulation is not needed in cases of thermal equilibrium so long as the basis states are chosen to be energy eigenstates. For most practical purposes, this can be taken as the fundamental definition of entropy since all other formulas for _S_can be mathematically derived from it, but not vice versa.

**Mean Field Theory**
Fast Fourier Transform
Euler’s formula: $e^{it}
